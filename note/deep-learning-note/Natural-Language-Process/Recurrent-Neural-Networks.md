# Recurrent Neural Networks

## 1. 为什么选择序列模型？(Why Sequence Models?)

语音识别:给定一个输入音频片段X，要求输出对应的文字记录Y。输入和输出数据都是序列模型，因为X是一个按时播放的音频片段，输出Y是一系列单词

音乐生成问题:只有输出数据Y是序列，而输入数据可以是空集，也可以是个单一的整数，这个数可能指代想要生成的音乐风格，或者想要生成的那首曲子的头几个音符

处理情感分类:输入数据X是序列，会得到类似这样的输入：“**There is nothing to like in this movie.**”，你认为这句评论对应几星？

**DNA**序列分析:**DNA**用**A**、**C**、**G**、**T**四个字母来表示。给定一段**DNA**序列，能够标记出哪部分是匹配某种蛋白质？

机器翻译:输入句:“**Voulez-vou chante avecmoi?**”(法语：要和我一起唱么？)，要求输出另一种语言的翻译结果

视频行为识别:得到一系列视频帧，要求识别其中的行为

命名实体识别:给定一个句子，要求识别出句中的人名

<div>
<img src="img/屏幕截图%202024-05-03%20152104.png" width=80%>
</div>

这些问题都可以被称作使用标签数据(X,Y)作为训练集的监督学习。但序列问题有很多不同类型。有些问题里，输入数据X和输出数据Y都是序列，但就算在那种情况下，X和Y可能会不一样长/一样长。在另一些问题里，只有X或者只有Y是序列

## 2. 数学符号(Notation)

序列模型的输入语句是：“Harry Potter and Herminoe Granger invented a new spell.”。假如想要建立一个能够自动识别句中人名位置的序列模型，那么这就是一个命名实体识别问题

<div>
<img src="img/屏幕截图%202024-05-03%20153515.png" width=45%>
<img src="img/屏幕截图%202024-05-03%20153528.png" width=46%>
</div>

该句话包含9个单词，输出y即为1x9向量，每位表征对应单词是否为人名的一部分，对应的输出y表示为:

$$
y=\begin{bmatrix}1&1&0&1&1&0&0&0&0\end{bmatrix}^T
$$

$y^{<t>}$ 表示序列对应位置的输出，$T_y$ 表示输出序列长度，$1\leq t \leq T_y$

对于输入x，表示为:

$$
y=\begin{bmatrix}x^{<1>}&x^{<2>}&x^{<3>}&x^{<4>}&x^{<5>}&x^{<16>}&x^{<7>}&x^{<8>}&x^{<9>}\end{bmatrix}^T
$$

$x^{<t>}$ 表示序列对应位置的输入，$T_x$ 表示输入序列长度。此例中 $T_x=T_y$，但是也存在 $T_x\neq T_y$

如何表示每个 $x^{<t>}$ ？

建立一个词汇库vocabulary，尽可能包含更多的词汇。例如一个包含10000个词汇的词汇库为:

$$
\begin{bmatrix}a&and&\cdots&harry&\cdots&potter&\cdots&zulu\end{bmatrix}^T
$$

然后使用one-hot编码，词汇表中与 $x^{<t>}$ 对应的位置为1，其它位置为0。如果出现词汇表之外的单词，可以使用UNK或其他字符串来表示(一个单词使用一个向量进行表示，这是因为如果只使用一个数字进行表示的话，就难以进行数组处理，我们要的是方便神经网络的训练！！！！！！！)

对于多样本:对应的命名规则可表示为:$X^{(i)<t>}$，$Y^{(i)<t>}$，i表示第i个样本。不同样本的 $T_x^{(i)}$ 或 $T_y^{(i)}$ 都有可能不同

## 3. 循环神经网络模型(Recurrent Neural Network Model)

序列模型从左到右，依次传递，此例中，$T_x=T_y$。$x^{<t>}$ 到 $\hat{y}^{<t>}$ 之间是隐藏神经元。$a^{<t>}$ 会传入到第t+1元素中，作为输入。其中，$a^{<0>}$一般为零向量

循环神经网络是从左向右扫描数据，同时每个时间步的参数也是共享的，用 $\bm{W}_{ax}$ 来表示管理着从 $x^{<1>}$ 到隐藏层的连接的一系列参数，每个时间步使用的都是相同的参数 $\bm{W}_{ax}$。而激活值是由参数 $\bm{W}_{aa}$ 决定，同时每一个时间步都使用相同的参数 $\bm{W}_{aa}$，同样的输出结果由 $\bm{W}_{ya}$决定:

<div>
<img src="img/屏幕截图%202024-05-03%20161304.png" width=45%>
<img src="img/屏幕截图%202024-05-03%20161530.png" width=45%>
</div>

预测 $\hat{y}^{<3>}$ 时，不仅要使用 $x^{<3>}$ 的信息，还要使用来自 $x^{<1>}$ 和 $x^{<2>}$ 的信息，而这个循环神经网络的一个缺点是只使用了这个序列中之前的信息来做出预测，因为如果给定了这个句子，“**Teddy Roosevelt was a great President.**”，为了判断**Teddy**是否是人名的一部分，仅仅知道句中前两个词是完全不够的，还需要知道句中后部分的信息，因为句子也可能是这样的，“**Teddy bears are on sale!**”。因此如果只给定前三个单词，是不可能确切地知道**Teddy**是否是人名的一部分，第一个例子是人名，第二个例子就不是，所以不可能只看前三个单词就能分辨出其中的区别

所以这样特定的神经网络结构的一个限制是它在某一时刻的预测仅使用了从序列之前的输入信息，并没有使用序列中后部分的信息

<div>
<img src="img/屏幕截图%202024-05-03%20161557.png" width=46.5%>
<img src="img/屏幕截图%202024-05-03%20163422.png" width=46%>
</div>

RNN的正向传播(Forward Propagation)过程为:

$$
a^{<t>}=g(\bm{W}_{aa}\cdot a^{<t−1>}+\bm{W}_{ax}\cdot x^{<t>}+b_a)
$$

$$
\hat{y}^{<t>}=g(W_{ya}\cdot a^{<t>}+b_y)
$$

为了简化表达式，可以对 $a^{<t>}$ 项进行整合:

$$
\bm{W}_{aa}\cdot a^{<t−1>}+\bm{W}_{ax}\cdot x^{<t>}=
\begin{bmatrix}\bm{W}_{aa}&\bm{W}_{ax}\end{bmatrix}\cdot
\begin{bmatrix}a^{<t−1>}\\a^{<t>}\end{bmatrix} \to
\bm{W}_a\cdot [a^{<t−1>},a^{<t>}]
$$

则正向传播可表示为:

$$
a^{<t>}=g(\bm{W}_a\cdot [a^{<t−1>},a^{<t>}]+b_a)
$$

$$
\hat{y}^{<t>}=g(W_{ya}\cdot a^{<t>}+b_y)
$$

## 4. 通过时间的反向传播(Backpropagation through time)

反向传播计算方向与前向传播基本上是相反:

<div>
<img src="img/屏幕截图%202024-05-03%20164702.png" width=46%>
<img src="img/屏幕截图%202024-05-03%20164647.png" width=46%>
</div>

识别人名的例子，经过RNN正向传播，单个元素的Loss function为:

$$
L^{<t>}(\hat{y}^{<t>},y^{<t>})=−y^{<t>}\log\hat{y}^{<t>}−(1−y^{<t>})\log(1−\hat{y}^{<t>})
$$

> 这是 binary classification 的 Loss Function，注意与softmax Loss Function区别

该样本所有元素的Loss function为:

$$
L(\hat{y},y)=\sum _{t=1}^{T_y}L^{<t>}(\hat{y}^{<t>},y^{<t>})
$$

反向传播(Backpropagation)过程就是从右到左分别计算 $L(\hat{y},y)$ 对参数 $W_a,W_y,b_a,b_y$ 的偏导数，这种从右到左的求导过程被称为Backpropagation through time

**RNN**反向传播示意图:

<div>
<img src="img/image_1.png" width=80%>
</div>

## 5. 不同类型的循环神经网络(Different types of RNNs)

RNN模型包含以下几个类型:

* 一对一，当去掉 $a^{<0>}$ 时它就是一种标准类型的神经网络
* 一对多，比如音乐生成或者序列生成
* 多对一，如是情感分类的例子，首先读取输入，一个电影评论的文本，然后判断他们是否喜欢电影还是不喜欢
* 多对多，如命名实体识别，$T_x=T_y$
* 多对多，如机器翻译，$Tx\neq Ty$

<div>
<img src="img/屏幕截图%202024-05-03%20170227.png" width=32%>
<img src="img/屏幕截图%202024-05-03%20170244.png" width=32.2%>
<img src="img/屏幕截图%202024-05-03%20170256.png" width=32%>
</div>

## 6. 语言模型和序列生成(Language model and sequence generation)

在语音识别中，某句语音有两种翻译:

* the apple and pair salad
* the apple and pear salad

语言模型会计算出这两句话各自的出现概率:

* P(The apple and pair salad)$=3.2\times10^{−13}$
* P(The apple and pear salad)$=5.7\times10^{−10}$

选择概率最大的语句作为正确的翻译

<div>
<img src="img/屏幕截图%202024-05-03%20172120.png" width=45%>
<img src="img/屏幕截图%202024-05-03%20172434.png" width=45%>
</div>

概率计算的表达式为:

$$
P(y^{<1>},y^{<2>},\cdots,y^{<T_y>})
$$

> 如何使用RNN构建语言模型:
>
> 首先需要一个足够大的训练集，训练集由大量的单词语句语料库(corpus)构成。然后，对corpus的每句话进行切分词(tokenize)，建立vocabulary，对每个单词进行one-hot编码。例如下面这句话:
>
> The Egyptian Mau is a bread of cat.
>
> 每句话结束末尾，需要加上< EOS >作为语句结束符。若语句中有词汇表中没有的单词，用< UNK >表示。假设单词“Mau”不在词汇表中，则上面这句话可表示为:
>
> The Egyptian < UNK > is a bread of cat. < EOS >
>
> 准备好训练集并对语料库进行切分词等处理之后，接下来构建相应的RNN模型:
>
> $x^{<1>}$ 和 $a^{<0>}$ 均为零向量。Softmax输出层 $\hat{y}^{<1>}$ 表示出现该语句第一个单词的概率，softmax输出层 $\hat{y}^{<2>}$ 表示在第一个单词基础上出现第二个单词的概率，即条件概率，以此类推，最后是出现< EOS >的条件概率
>
> 注意:这个过程会把 $y^{<t-1>}$ 赋值给 $x^{<t>}$ !!!!!!!

单个元素的softmax loss function为:

$$
L^{<t>}(\hat{y}^{<t>},y^{<t>})=−\sum_iy_i^{<t>}\log \hat{y}_i^{<t>}
$$

该样本所有元素的Loss function为:

$$
L(\hat{y},y)=\sum_tL^{<t>}(\hat{y}^{<t>},y^{<t>})
$$

对语料库的每条语句进行RNN模型训练，最终得到的模型可以根据给出语句的前几个单词预测其余部分，将语句补充完整。例如给出“Cats average 15”，RNN模型可能预测完整的语句是“Cats average 15 hours of sleep a day.”

整个语句出现的概率等于语句中所有元素出现的条件概率乘积(输出的值是在前面的单词出现的前提之下的概率)。例如某个语句包含 $y^{<1>},y^{<2>},y^{<3>}$，则整个语句出现的概率为:

$$
P(y^{<1>},y^{<2>},y^{<3>})=P(y^{<1>})\cdot P(y^{<2>}|y^{<1>})\cdot P(y^{<3>}|y^{<1>},y^{<2>})
$$

## 7. 对新序列采样(Sampling novel sequences)

基于词汇的RNN模型

序列模型模拟了任意特定单词序列的概率，要做的是对这些概率分布进行采样来生成一个新的单词序列。网络已经被训练过，下面进行的是采样

第一步要做的是对想要模型生成的第一个词进行采样，输入 $x^{<1>}=0,a^{<0>}=0$，第一个时间步得到的是所有可能的输出，是经过softmax层后得到的概率，然后根据这个softmax的分布进行随机采样。对这个向量使用np.random.choice，来根据向量中这些概率的分布进行采样，就能对第一个词进行采样

然后继续下一个时间步，$\hat{y}^{<1>}$ 作为输入，然后softmax层就会预测 $\hat{y}^{<2>}$ 是什么。假如第一个词进行抽样后得到的是The，现在要计算出在第一词是The的情况下，第二个词应该是什么，然后再用采样函数来对 $\hat{y}^{<2>}$ 进行采样

即无论得到什么样的用one-hot码表示的选择结果，都把它传递到下一个时间步，然后进行采样，直到最后一个时间步

怎样知道一个句子结束:

* 如果代表句子结尾的标识在字典中，可以一直进行采样直到得到EOS标识，代表着已经抵达结尾，可以停止采样
* 如果字典中没有这个词，可以决定从20个或100个或其他个单词进行采样，然后一直将采样进行下去直到达到所设定的时间步。不过这种过程有时候会产生一些未知标识，如果要确保算法不会输出这种标识，要做的是拒绝采样过程中产生任何未知的标识，一旦出现就继续在剩下的词中进行重采样，直到得到一个不是未知标识的词

这就是如何从RNN语言模型中生成一个随机选择的句子。以上所建立的是基于词汇的RNN模型，意思就是字典中的词都是英语单词

<div>
<img src="img/屏幕截图%202024-05-03%20174252.png" width=45%>
<img src="img/屏幕截图%202024-05-03%20174726.png" width=46%>
</div>

基于字符的RNN结构(用字符组成字典)

序列 $y^{<1>},y^{<2>},y^{<3>}$ 在训练数据中是单独的字符，对于“Cats average 15 hours of sleep a day.”，C是 $y^{<1>}$，a就是 $y^{<2>}$，t就是 $y^{<3>}$ 等等

> 优点:
> 不必担心会出现未知的标识，基于字符的语言模型会将Mau这样的序列也视为可能性非零的序列。而基于词汇的语言模型，如果Mau不在字典中，只能当作未知标识UNK
> 缺点:
> 最后会得到太多太长的序列，基于字符的语言模型在捕捉句子中的依赖关系也就是句子较前部分如何影响较后部分不如基于词汇的语言模型那样可以捕捉长范围的关系，并且基于字符的语言模型训练起来计算成本比较高

### 说明：

np.random.choice(a,size=None,replace=True,p=None) -> 从原始矩阵取到的值的数组
该函数表示从一个数组中按照每个位置的概率(权重)取若干个值，得到一个数值数组。该函数可以在需要根据概率取值的时候用，非常好用。可以让你每次都不会固定取到某个值，而是概率大的取到的值得概率大

> 参数说明:
>
> * a:原始取值矩阵
> * size:取值得个数，int；如果replace为False，大小必须和a和p一致；如果replace为True，大小随意
> * replace:默认True,bool类型，表示有放回，就是有可能会取到相同的数据，如果设置为False，则不会取到重复的数据
> * p:每个值对应的概率，和a的长度必须一样

## 8. 循环神经网络的梯度消失(Vanishing gradients with RNNs)

<div>
<img src="img/屏幕截图%202024-05-03%20180119.png" width=80%>
</div>

cat是单数，应该用was，cats是复数，用were

这个例子中的句子有长期的依赖，最前面的单词对句子后面的单词有影响。但基本的RNN模型不擅长捕获长期依赖效应

RNN反向传播很困难，会有梯度消失的问题，后面层的输出误差很难影响前面层的计算。即很难让一个神经网络能够意识到它要记住看到的是单数名词还是复数名词，然后在序列后面生成依赖单复数形式的was或者were。且在英语里面中间的内容可以任意长，所以基本的RNN模型会有很多局部影响，输出 $\hat{y}^{<3>}$ 主要受附近的值的影响，最终的输出很难受到序列靠前的输入的影响，因为不管输出是什么，对的还是错的，这个区域都很难反向传播到序列的前面部分，也因此网络很难调整序列前面的计算

在反向传播的时候，随着层数的增多，梯度不仅可能指数型的下降，也可能指数型的上升。梯度消失在训练RNN时是首要的问题，不过梯度爆炸也会出现，但是梯度爆炸很明显，因为指数级大的梯度会让参数变得极其大，以至于网络参数崩溃。参数大到崩溃会看到很多NaN，或者不是数字的情况，这意味着网络计算出现了数值溢出

解决方法:用梯度修剪。梯度向量如果大于某个阈值，缩放梯度向量，保证不会太大
