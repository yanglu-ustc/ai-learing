# Sequence models & Attention mechanism

## 1. 基础模型(Basic Models)

<div>
<img src="img/屏幕截图%202024-05-07%20212533.png" width=45%>
<img src="img/屏幕截图%202024-05-07%20212544.png" width=45%>
</div>

### 机器翻译

用 $x^{<1>}$ 到 $x^{<5>}$ 表示输入句子的单词，用 $y^{<1>}$ 到 $y^{<6>}$ 表示输出句子的单词

首先，建立一个**RNN**编码网络(**encoder network**)，单元可以是**GRU**或**LSTM**。每次只向该网络中输入一个法语单词，将输入序列接收完毕后，这个**RNN**网络会输出一个向量来代表这个输入序列

之后建立一个解码网络，以编码网络的输出作为输入，之后它可以被训练为每次输出一个翻译后的单词，一直到它输出序列的结尾或者句子结尾标记

在给出足够的法语和英语文本的情况下，训练模型，通过输入一个法语句子来输出对应的英语翻译，这个模型将会非常有效。这个模型简单地用一个编码网络来对输入的法语句子进行编码，然后用一个解码网络来生成对应的英语翻译

### 图像描述

> 给出猫的图片，能自动地输出该图片的描述:一只猫坐在椅子上

通过输入图像来输出描述:

将图片输入到卷积神经网络中(一个预训练的**AlexNet**结构)，然后让其学习图片的编码，或者学习图片的一系列特征。去掉最后的**softmax**单元，这个预训练的**AlexNet**结构会输出4096维的特征向量，向量表示的就是这只猫的图片，这个预训练网络可以是图像的编码网络

接着把这个向量输入到**RNN**中，RNN要做的就是生成图像的描述，每次生成一个单词:输入一个描述输入的特征向量，然后让网络一个一个地输出单词序列

## 2. 选择最可能的句子(Picking the most likely sentence)

**seq2seq**机器翻译模型和第一周所用的语言模型之间有很多相似的地方，但也有许多重要的区别:

可以把机器翻译想成是建立一个条件语言模型，能够估计句子的可能性

<div>
<img src="img/屏幕截图%202024-05-07%20213536.png" width=45%>
<img src="img/屏幕截图%202024-05-07%20213548.png" width=45%>
</div>

绿色表示**encoder**网络，紫色表示**decoder**网络。不同在于语言模型总是以零向量开始，而**encoder**网络会计算出一系列向量来表示输入的句子，以这个向量作为输入，这叫做条件语言模型(**conditional language model**)

相比语言模型输出任意句子的概率，翻译模型会输出句子的英文翻译，这取决于输入的法语句子。即估计一个英文翻译的概率，比如估计"**Jane is visiting Africa in September.**"翻译的概率，这句翻译是取决于法语句子，"**Jane visite I'Afrique en septembre.**"，这就是英语句子相对于输入的法语句子的可能性，是一个条件语言模型

> 模型将法语翻译成英文，通过输入的法语句子模型将会告诉你各种英文翻译所对应的可能性

x是法语句子"**Jane visite l'Afrique en septembre.**"，它将告诉你不同的英语翻译所对应的概率:从这个分布中进行取样得到 $P(y|x)$，但不是从得到的分布中进行随机取样，而是要找到一个英语句子y，使得条件概率最大化:

$$
\max P(y^{<1>},y^{<2>},\cdots,y^{<T_y>}|x^{<1>},x^{<2>},\cdots,x^{<T_x>})
$$

而解决这种问题最通用的算法就是束搜索(**Beam Search**)

为什么不用贪心搜索(**Greedy Search**):

贪心搜索生成第一个词的分布以后，将会根据条件语言模型挑选出最有可能的第一个词进入机器翻译模型中，在挑选出第一个词之后将会继续挑选出最有可能的第二个词...这种算法就叫做贪心搜索，但是真正需要的是一次性挑选出整个单词序列，从 $y^{<1>}$、$y^{<2>}$ 到 $y^{<T_y>}$ 来使得整体的概率最大化。所以贪心算法先挑出最好的第一个词，在这之后再挑最好的第二词，然后再挑第三个，这种方法并不管用

<div>
<img src="img/屏幕截图%202024-05-07%20213707.png" width=80%>
</div>

第一串翻译明显比第二个好，但如果贪心算法挑选出了"**Jane is**"作为前两个词，因为在英语中**going**更加常见，于是对于法语句子来说"**Jane is going**"相比"**Jane is visiting**"会有更高的概率作为法语的翻译，所以如果仅仅根据前两个词来估计第三个词的可能性，得到的更可能是**going**，最终得到一个欠佳的句子

当想得到单词序列 $y^{<1>}$、$y^{<2>}$ 一直到最后一个词总体的概率时，一次仅仅挑选一个词并不是最佳的选择。如果字典中有10,000个单词，翻译有10个词，可能的组合就有10,000的10次方这么多，从这样大一个字典中来挑选单词，句子数量非常巨大，大大增加了运算成本，降低运算速度，不可能去计算每一种组合的可能性

所以最常用的办法就是用一个近似的搜索算法，它会尽力地将挑选出句子y使得条件概率最大化，尽管不能保证找到的y值一定可以使概率最大化

机器翻译模型和之前的语言模型一个主要的区别就是:相比之前的模型随机地生成句子，该模型是找到最有可能的翻译

## 3. 集束搜索(Beam Search)

“**Jane visite l'Afrique en Septembre.**”翻译成英语"**Jane is visiting Africa in September**".，集束搜索算法首先做的就是挑选要输出的英语翻译中的第一个单词。这里列出了10,000个词的词汇表，忽略大小写，在集束搜索的第一步中用这个网络来评估第一个单词的概率值，给定输入序列x**x**，即法语作为输入，第一个输出y**y**的概率值是多少

[![](https://baozoulin.gitbook.io/~gitbook/image?url=https%3A%2F%2Fgithub.com%2Ffengdu78%2Fdeeplearning_ai_books%2Fraw%2Fmaster%2Fimages%2F8a22dfb5d3c0a4b5d2fdfa716dc3f3b2.png&width=300&dpr=4&quality=100&sign=936588d397222a5f1b653827b7831a7be49430baaee82e1e9f364f0101317d43)](https://github.com/fengdu78/deeplearning_ai_books/blob/master/images/8a22dfb5d3c0a4b5d2fdfa716dc3f3b2.png)

集束搜索会考虑多个选择，集束搜索算法会有一个参数**B**，叫做集束宽（**beam width**）。这个例子中集束宽设成3，意味着集束搜索一次会考虑3个可能结果，比如对第一个单词有不同选择的可能性，最后找到**in**、**jane**、**september**，是英语输出的第一个单词的最可能的三个选项，然后集束搜索算法会把结果存到计算机内存里以便后面尝试用这三个词。为了执行集束搜索的第一步，需要输入法语句子到编码网络，然后解码这个网络，**softmax**层会输出10,000个概率值，然后取前三个存起来，概率表示为：

P(y^<1>∣x)**P**(**y**^****<**1**>**∣**x**)**

集束搜索算法的第二步：针对每个第一个单词考虑第二个单词是什么

[![](https://baozoulin.gitbook.io/~gitbook/image?url=https%3A%2F%2Fgithub.com%2Ffengdu78%2Fdeeplearning_ai_books%2Fraw%2Fmaster%2Fimages%2F14a940ae2ea7932b7b7190eceb79f79e.png&width=300&dpr=4&quality=100&sign=5a7547d83eb9fa0a625e22c8b127b497ec679ec8ac4141345b75654ad956f5f9)](https://github.com/fengdu78/deeplearning_ai_books/blob/master/images/14a940ae2ea7932b7b7190eceb79f79e.png)

为了评估第二个词的概率值，把y<1>**y**<**1**>设为单词**in**（编号3），输出就是y<2>**y**<**2**>（编号5），有了这个连接（编号6），这个网络就可以用来评估：在给定法语句子和翻译结果的第一个单词**in**的情况下第二个单词的概率

在第二步更关心的是要找到最可能的第一个和第二个单词对，所以不仅仅是第二个单词有最大的概率，而是第一个、第二个单词对有最大的概率（编号7）。可以表示成第一个单词的概率（编号8）乘以第二个单词的概率（编号9）：

P(y^<1>,y^<2>∣x)=P(y^<1>∣x)⋅P(y^<2>∣x,y^<1>)**P**(**y**^****<**1**>**,**y**^****<**2**>**∣**x**)**=**P**(**y**^****<**1**>**∣**x**)**⋅**P**(**y**^****<**2**>**∣**x**,**y**^****<**1**>**)**

**jane**、**september**跟上面一样

[![](https://baozoulin.gitbook.io/~gitbook/image?url=https%3A%2F%2Fgithub.com%2Ffengdu78%2Fdeeplearning_ai_books%2Fraw%2Fmaster%2Fimages%2F507c9081ee77c686bb96a009248087fd.png&width=300&dpr=4&quality=100&sign=62f2ba6ea800e0d5c997d579397512f10a408e711247783c50903b629144d03a)](https://github.com/fengdu78/deeplearning_ai_books/blob/master/images/507c9081ee77c686bb96a009248087fd.png)

注意，如果集束搜索找到了第一个和第二个单词对最可能的三个选择是“**in September**”或者“**jane is**”或者“**jane visits**”，就意味着去掉了**september**作为英语翻译结果的第一个单词的选择，第一个单词现在减少到了两个可能结果，但是集束宽是3，还是有y<1>**y**<**1**>，y<2>**y**<**2**>对的三个选择

接着，再预测第三个单词。分别以in september，jane is，jane visits为条件，计算每个词汇表单词作为预测第三个单词的概率。从中选择概率最大的3个作为第三个单词的预测值，得到：in september jane，jane is visiting，jane visits africa

[![](https://baozoulin.gitbook.io/~gitbook/image?url=https%3A%2F%2Fgithub.com%2Ffengdu78%2Fdeeplearning_ai_books%2Fraw%2Fmaster%2Fimages%2F6a0b785dd54fcbc439bd82794eeefcf8.png&width=300&dpr=4&quality=100&sign=af916832dc622bc1730e49c6ab0beecf999a84c8e48c0e24bffaaa8779b95e0c)](https://github.com/fengdu78/deeplearning_ai_books/blob/master/images/6a0b785dd54fcbc439bd82794eeefcf8.png)

概率表示为：

P(y^<3>∣x,y^<1>,y^<2>)**P**(**y**^****<**3**>**∣**x**,**y**^****<**1**>**,**y**^****<**2**>**)**

此时，得到的前三个单词的3种情况的概率为：

P(y^<1>,y^<2>,y^<3>∣x)=P(y^<1>∣x)⋅P(y^<2>∣x,y^<1>)⋅P(y^<3>∣x,y^<1>,y^<2>)**P**(**y**^****<**1**>**,**y**^****<**2**>**,**y**^****<**3**>**∣**x**)**=**P**(**y**^****<**1**>**∣**x**)**⋅**P**(**y**^****<**2**>**∣**x**,**y**^****<**1**>**)**⋅**P**(**y**^****<**3**>**∣**x**,**y**^****<**1**>**,**y**^****<**2**>**)

以此类推，每次都取概率最大的三种预测。最后，选择概率最大的那一组作为最终的翻译语句：

**Jane is visiting Africa in September.**

如果参数B=1，则就等同于greedy search。实际应用中，可以根据不同的需要设置B为不同的值。一般B越大，机器翻译越准确，但同时也会增加计算复杂度
