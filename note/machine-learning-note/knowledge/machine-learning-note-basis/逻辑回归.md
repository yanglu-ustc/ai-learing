# logistic regression

+ ues of classification

## binary classification (class = catagory)

+ true or false........
+ negative class(false) & positive class(true)

## sigmoid function/logistic function

+ outputs between 0 and 1

> $$
> g(z)=\frac{1}{1+e^{-z}} \quad \quad 0<g(z)<1
> $$
>
> $$
> z=\vec{w}\vec{x}+b \longrightarrow g(z)=\frac{1}{1+e^{-z}}
> $$
>
> $$
> f_{\vec{w},b}(\vec{x})=g(\vec{w}\vec{x}+b)=\frac{1}{1+e^{-(\vec{w}\vec{x}+b)}}=P(y=1|x;\vec{w},b)
> $$

+ Decision Boundary

> Yes:$\hat{y}=1$,No:$\hat{y}=0$
>
> when is $f_{\vec{w},b}(\vec{x}) \geq 0.5$ ?
>
> $$
> g(z) \geq 0.5 \longrightarrow z \geq 0 \longrightarrow \vec{w}\vec{x}+b \geq 0\longrightarrow \hat{y}=1
> $$
>
> $z=\vec{w}\vec{x}+b$ is the Decision Boundary

+ cost function for logistic regression

> $$
> L(\theta)=\prod_{i=1}^{m}p(y^{(i)}|\vec{x}^{(i)};\theta)=\prod_{i=1}^{m}(h_{\theta}(\vec{x}^{(i)}))^{y^{(i)}}(1-h_{\theta}(\vec{x}^{(i)}))^{1-y^{(i)}}
> $$
>
> $$
> l(\theta)=\log L(\theta)=\sum_{i=1}^{m}(y^{(i)}\log h_{\theta}(\vec{x}^{(i)})+(1-y^{(i)})\log (1-h_{\theta}(\vec{x}^{(i)}))
> $$
>
> $$
> J(\theta)=-\frac{1}{m}l(\theta) \quad 为了实现梯度下降.....会带上一个负号！！！！！！！
> $$
>
> $$
> J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log h_{\theta}(\vec{x}^{(i)})+(1-y^{(i)})\log (1-h_{\theta}(\vec{x}^{(i)})] \quad h_{\theta}(x^{(i)})=f_{\vec{w},b}(\vec{x}^{(i)})
> $$
>
> $$
> \frac{\partial}{\partial w_{j}}J(\vec{w},b)=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\frac{1}{f_{\vec{w},b}(\vec{x}^{(i)})}\frac{\partial}{\partial w_{j}}f_{\vec{w},b}(\vec{x}^{(i)})-(1-y^{(i)})\frac{1}{1-f_{\vec{w},b}(\vec{x}^{(i)})}\frac{\partial}{\partial w_{j}}f_{\vec{w},b}(\vec{x}^{(i)}]
> =-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\frac{1}{f_{\vec{w},b}(\vec{x}^{(i)})}-(1-y^{(i)})\frac{1}{1-f_{\vec{w},b}(\vec{x}^{(i)})}]\frac{\partial}{\partial w_{j}}f_{\vec{w},b}(\vec{x}^{(i)})
> $$
>
> $$
> \frac{\partial}{\partial w_{j}}f_{\vec{w},b}(\vec{x}^{(i)})=\frac{\partial}{\partial w_{j}}\frac{1}{1+e^{-(\vec{w}\vec{x}^{(i)}+b)}}=-\frac{1}{(1+e^{-(\vec{w}\vec{x}^{(i)}+b)})^{2}}(-e^{-(\vec{w}\vec{x}^{(i)}+b)})x_{j}^{(i)}=f_{\vec{w},b}(\vec{x}^{(i)})(1-f_{\vec{w},b}(\vec{x}^{(i)}))x_{j}^{(i)}
> $$
> 
+ the same conclusion as linear regression:
>
> $$
> the \ conclusion : \ \frac{\partial}{\partial w_{j}}J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(x^{(i)})-y^{(i)})x_{j}^{(i)} \quad \frac{\partial}{\partial b}J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(x^{(i)})-y^{(i)})
> $$
