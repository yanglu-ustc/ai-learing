# The problem of Overfitting

+ example：二次函数的拟合点集：
  + 一维线性拟合：underfit
  + 四维....：overfit —— 过拟合
  + overfit or High variance：高方差/过拟合
  + underfit or high bias：高偏差/欠拟合
+ ways
  + Collect more training examples
  + Select features to include/exclude
  + Regularization

## Regularization

$$
J'(\vec{w},b)=J(\vec{w},b)+\frac{\lambda}{2m}\sum_{j=1}^{n}\vec{w_{j}}^{2}
$$

+ $\frac{\lambda}{2m}\sum_{j=1}^{n}\vec{w}^{2}$：这一项被称为惩罚项，这是为了阻止参数过多，或者某些参数的值对拟合的结果影响过大！！！！！

$$
\underset{\vec{w},b}{min}J'(\vec{w},b)=\underset{\vec{w},b}{min}[J(\vec{w},b)+\frac{\lambda}{2m}\sum_{j=1}^{n}\vec{w_{j}}^{2}]
$$

+ 两个极端
  + $\lambda\to +\infty:f(\vec{w},b)=b$
  + $\lambda\to 0:f(\vec{w},b)=xxx+xxx+\dots+xxx+xxx$

> $$
> \underset{\vec{w},b}{min}J'(\vec{w},b)=\underset{\vec{w},b}{min}[\frac{1}{2m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum_{j=1}^{n}\vec{w_{j}}^{2}]
> $$
>
> + 注意：$b$是没有惩罚项的！！！！！
>
> $$
> \frac{\partial}{\partial w_{j}}J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(x^{(i)})-y^{(i)})x_{j}^{(i)}+\frac{\lambda}{m}w_{j} \quad \frac{\partial}{\partial b}J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(x^{(i)})-y^{(i)})
> $$
>
> $$
> w_{j}=w_{j}-\alpha[\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(x^{(i)})-y^{(i)})x_{j}^{(i)}+\frac{\lambda}{m}w_{j}] \quad b=b-\alpha\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(x^{(i)})-y^{(i)})
> $$
>
> + 更为直观的来说：每次操作都是在缩小$w_{j}$的值
>
> $$
> w_{j}=w_{j}(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(x^{(i)})-y^{(i)})x_{j}^{(i)} \quad b=b-\alpha\frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(x^{(i)})-y^{(i)})
> $$

## use in the logistic regression

$$
J(\vec{w},b)=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log f_{\vec{w},b}(\vec{x}^{(i)})+(1-y^{(i)})\log (1-f_{\vec{w},b}(\vec{x}^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^{n}\vec{w_{j}}^{2}
$$

the conclusion is same to the i
