# 线性回归

+ 数学

$$
拟合的平面：h_{\theta}(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}（\theta_{0}是偏置项）\newline
整合：h_{\theta}(x)=\sum_{i=0}^{n}\theta_{i}x_{i}=\theta^{T}x
$$

+ 误差 —— 对于每个样本：

  $$
  y^{(i)}=\theta^{T}x^{(i)}+\varepsilon^{(i)}
  $$
+ 误差分析

  + 误差$\varepsilon^{(i)}$是独立具有相同的分布，并且服从均值为0的方差为$\theta^{2}$的高斯分布
  + 预测值与误差：$y^{(i)}=\theta^{T}x^{(i)}+\varepsilon^{(i)}$
  + 由于误差服从高斯分布：$p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(\epsilon^{(i)})^{2}}{2\sigma ^{2}})$
  + 带入之后有：

    $$
    p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^{T}x^{(i)})^{2}}{2\sigma ^{2}})
    $$
  + 似然函数：什么样的参数跟我们的参数组合后恰好是真实值

    $$
    L_{\theta}=\prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\theta)=\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^{T}x^{(i)})^{2}}{2\sigma ^{2}})
    $$
  + 对数似然：乘法转加法.......

    $$
    \log L_{\theta}=\log \prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^{T}x^{(i)})^{2}}{2\sigma ^{2}})=\sum_{i=1}^{m}\log \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^{T}x^{(i)})^{2}}{2\sigma ^{2}})=m\log \frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{\sigma ^{2}}\frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\theta^{T}x^{(i)})^{2}
    $$

    $$
    J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\theta^{T}x^{(i)})^{2}
    $$
  + 目标函数的求解：

    $$
    J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2}=\frac{1}{2}(X\theta-y)^{T}(X\theta-y)
    $$

    $$
    \nabla_{\theta}J(\theta)=\nabla_{\theta}(\frac{1}{2}(X\theta-y)^{T}(X\theta-y))=\nabla_{\theta}(\frac{1}{2}(\theta^{T}X^{T}-y^{T})(X\theta-y))=\nabla_{\theta}(\frac{1}{2}(\theta^{T}X^{T}X\theta-\theta^{T}X^{T}y-y^{T}X\theta+y^{T}y))=X^{T}X\theta-X^{T}y=0
    $$

    $$
    \theta=(X^{T}X)^{-1}X^{T}y
    $$
+ 我们的目的是为了拟合一个：$y^{(i)}=f_{w,b}(x^{(i)})$，使得下面的这个式子的值最小

  > model:$f_{w,b}(x)=wx+b$
  >
  > parameters:$w,b$
  >
  > cost function:
  >

  $$
  J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})^2
  $$

  > goal:$\underset{w,b}{minimize}J(w,b)$
  >
  > simplified:$f_{w}=wx$
  >

  $$
  J(w)=\frac{1}{2m}\sum_{i=1}^{m}(f_{w}(x^{(i)})-y^{(i)})^2
  $$
+ cost function：呈现一个位于空间中的U型图像，这是三个维度，是一个$J$关于$w$和$m$的函数

## 梯度下降模型

+ Have some function $J(w,b)$
+ Want $\underset{w,b}{min}J(w,b)$
+ the more common situation:

  $$
  \underset{w_{1},w_{2},...,w_{n},b}{min}J(w_{1},w_{2},...,w_{n},b)
  $$

  > $$
  > w=w_{0}-\alpha \frac{\partial}{\partial w}J(w,b) \quad b=b_{0}-\alpha \frac{\partial}{\partial b}J(w,b)
  > $$
  >
  > $$
  > \alpha:learning \ rate
  > $$
  >
+ the choice of learning rate

  + if $\alpha$ is too small...the gradient descent may be slow
  + if $\alpha$ is too large...the gradient descent may: - overshoot,never reach minimum
+ Can reach local minimum with fixed learning rate

  + Near a local minimum

    + Derivative becomes smaller
    + Updater steps become smaller
    + Can reach local minimum without decreasing learning rate

  > $$
  > \frac{\partial}{\partial w}J(w,b)=\frac{\partial}{\partial w}\frac{1}{2m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})^2=\frac{\partial}{\partial w}\frac{1}{2m}\sum_{i=1}^{m}(wx^{(i)}+b-y^{(i)})^2=\frac{1}{m}\sum_{i=1}^{m}(wx^{(i)}+b-y^{(i)})x^{(i)}=\frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})x^{(i)}
  > $$
  >
  > $$
  > \frac{\partial}{\partial b}J(w,b)=\frac{\partial}{\partial b}\frac{1}{2m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})^2=\frac{\partial}{\partial b}\frac{1}{2m}\sum_{i=1}^{m}(wx^{(i)}+b-y^{(i)})^2=\frac{1}{m}\sum_{i=1}^{m}(wx^{(i)}+b-y^{(i)})=\frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})
  > $$
  >
  > $$
  > w=w_{0}-\alpha \frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})x^{(i)} \quad b=b_{0}-\alpha \frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})
  > $$
  >
+ batch gradient descent

  + batch:Each step of gradient decent uses all the training examples
  + 更加普遍的情况：

    $$
    \frac{\partial}{\partial \theta_{j}}J(\theta)=\frac{\partial}{\partial \theta_{j}}\frac{1}{2m}\sum_{i=1}^{m}(f_{\theta}(x^{(i)})-y^{(i)})^2=\frac{\partial}{\partial \theta_{j}}\frac{1}{2m}\sum_{i=1}^{m}(\theta_{0}x_{0}+\theta_{1}x_{1}+...+\theta_{n}x_{n}-y^{(i)})^2=\frac{1}{m}\sum_{i=1}^{m}(\theta_{0}x_{0}+\theta_{1}x_{1}+...+\theta_{n}x_{n}-y^{(i)})x_{j}
    $$

    $$
    \frac{\partial}{\partial \theta_{j}}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(f_{\theta}(x^{(i)})-y^{(i)})x_{j} \quad the \ important \ thing \ is \ that \ [x_{0} \equiv 1]
    $$
