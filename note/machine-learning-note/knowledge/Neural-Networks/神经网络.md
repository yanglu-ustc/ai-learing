# 神经网络

+ 神经网络(Neural Networks)
  + inference(prediction)
  + train
+ Practical advice for building machine learning system
+ Decision Trees（决策树）

## 神经元与大脑

## Demand Prediction

top seller or not ?

$$
a=f_{\vec{w},b}(\vec{x})=\frac{1}{1+e^{-(\vec{w}\vec{x}+b)}} \quad a:“activation”
$$

+ a refers to how much a nenron is sending a high output to other nenrons downstream from it
+ the little logistic regression can be thought a very simplified model of a single neuron in the brain

> each one > is a layer
>
>> this layer can be told "activation values"
>>
>> price
>> shipping cost
>> marketing
>> material
>>
>>> affordability(price,shipping cost)
>>> awareness(marketing)
>>> perceived quality(price,material)
>>>
>>>> probality of being a top seller(affordability,awareness,perceived quality)
>>>>
>>>
>>

+ input layer --> hidden layer --> ... -->hidden layer --> output layer
+ mutilayer perceptron

## Nenural network layer

$$
\vec{w}_{j}^{[i]} \quad b_{j}^{[i]} \quad \vec{a}_{j}^{[i]}:表示的是第i层layer的特征值、输出值，这个j一般用来表示第j个范例元素
$$

$$
\vec{x} \to \vec{a}^{[1]} \to \vec{a}^{[2]} \to \dots \to \vec{a}^{[n]}
$$

+ 这里的$k$是第$n$层layer第$k$个神经元，每一个神经元的$\vec{w}$不同，与前面一层的layer决定最终的值
  + $g()$又被称为"activation function"

$$
a_{k}^{[n]}=g(\vec{w}_{k}^{[n]}\vec{a}^{[n-1]}+b_{k}^{[n]})
$$

## forward propagation

+ 神经网络向前传播算法

> ```python
> # 描写的是一个矩阵
> x = np.array([[200.0,17.0]])
> # units:the number of the hidden units
> layer_1 = Dense(units=3,activation='sigmoid')
> a_1 = layer_1(x)
> layer_2 = Dense(units=1,activation='sigmoid')
> a_2 = layer_2(a_1)
> if a_2 >= 0.5:
>   yhat = 1
> else:
>   yhat = 0
> ```
>
> ```python
> # Tensorflow
> tf.Tensor([0.8], shape=(1,1), dtype=float32)
> # Numpy
> np.array([0.8], dtype=float32)
> ```

## Building a neural network

> ```python
> import numpy as np
> layer_1 = Dense(units=3,activation='sigmoid')
> layer_2 = Dense(units=1,activation='sigmoid')
> model = Sequential([layer_1, layer_2])
> x = np.array([200.0,17.0],[120.0,5.0],[425.0,20.0],[212.0,18.0])
> # targets
> y = np.array([1,0,0,1])
>
> model.compile(...)
> model.fit(x,y)
> model.predict(x_new)
> ```
> + $W$的每一列都是一个向量，代表的是这一层的神经元的特征向量
>
> $$
> W=\begin{bmatrix}w_{11} & w_{21} & w_{31} & \dots & w_{k_{n}1}\\ w_{12} & w_{22} & w_{32} & \dots & w_{k_{n}2}\\ \dots & \dots & \dots & \dots & \dots\\ w_{1k_{n-1}} & w_{2k_{n-1}} & w_{3k_{n-1}} & \dots & w_{k_{n}k_{n-1}}\\\end{bmatrix}
> \quad
> \vec{a}^{[n]}=\begin{bmatrix} a^{[n]}_{1} & a^{[n]}_{2} & \dots & a^{[n]}_{k_{n}} \\ \end{bmatrix}
> \quad
> \vec{b}^{[n]}=\begin{bmatrix}b_{1} & b_{2} & b_{3} & \dots & b_{k_{n}} \\ \end{bmatrix}
> $$
>
> $$
> \vec{a}^{[n]}=g(\vec{a}^{[n-1]}\cdot W+\vec{b}^{[n]})
> $$
>
> ```python
> def dense(a_in,W,b,g):
>   units = W.shape[1]
>   a_out = np.zeros(units)
>   for j in range(units):
>       # w取的是所有行的第j列的数据
>       w = W[:,j]
>       # 这里使用的是向量的点积，不是矩阵乘法
>       z = np.dot(w,a_in) + b[j]
>       a_out[j] = g(z)
>   return a_out
> 
> def dense(a_in,W,b,g):
>   z = np.matmul(a_in,W) + b
>   # 矩阵运算的优势，将矩阵的每一个元素进行运算并传入数据
>   a_out = g(z)
>   return a_out
> 
> def sequential(x):
>   a1 = dense(x,w1,b1)
>   a2 = dense(a1,w1,b2)
>   a3 = dense(a2,w3,b3)
>   a4 = dense(a3,w4,b4)
>   f_x = a4
>   return  f_x
> ```
